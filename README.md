# RAVEface_V1


RAVEface explores an open-ended mapping between fa- cial features and generative neural audio. It offers an ex- ploratory and visual controller for timbral space, where abstract but generative textures arise from the interplay of gesture and learned sound representation through a real- time variational autoencoder (VAE). By streaming 3D fa- cial landmarks from Google MediaPipe to Cyclying’ 74 Max/MSP running multiple RAVE models (self-trained and pre-trained), the system transforms the performer’s face into a multidimensional latent space manipulator that results in an abstract but definable neural-synthesis based soundscape.
